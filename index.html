<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CZ-Net</title>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

<link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
rel="stylesheet">

<link rel="stylesheet" href="./static/css/bulma.min.css">
<link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
<link rel="stylesheet" href="./static/css/bulma-slider.min.css">
<link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
<link rel="stylesheet"
href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
<link rel="stylesheet" href="./static/css/index.css">
</head>

<head>
<style  type="text/css">
	.slider-pagination .slider-page {
		background-color: #201e1e !important;
	}
	</style>
</head>>

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<title>CZ-Net</title>
	<link rel="stylesheet" href="bootstrap/css/bootstrap.min.css">
	<script src="bootstrap/js/jquery.min.js"></script>
	<link rel="stylesheet" href="bootstrap/js/bootstrap.min.js">
</head>

<body>
	<div class="container">
		<div class="content">
			<h1 style="text-align:center; margin-top:60px; font-weight: bold; font-size: 50px">
				CrossZoom: Simultaneously Motion Deblurring and Event Super-Resolving 
			</h1>
			<p style="text-align:center; margin-bottom:15px; margin-top:20px; font-size: 18px">
				<a>Chi Zhang<sup>1</sup></a>,
				<a href="https://xiangz-0.github.io/" target="_blank">Xiang Zhang<sup>2</sup></a>,
				<a href="https://mingyuan-lin.github.io/" target="_blank">Mingyuan Lin<sup>1</sup></a>,
				<a>Cheng Li<sup>3</sup></a>,
				<a>Chu He<sup>1</sup></a>,
				<a>Wen Yang<sup>1</sup></a>,
				<a href="http://www.captain-whu.com/team.html" target="_blank">Gui-Song Xia<sup>4</sup></a>,
				<a href="https://dvs-whu.cn/" target="_blank">Lei Yu<sup>1,*</sup></a>
			</p>
			<p style="text-align:center; margin-bottom:15px; margin-top:20px; font-size: 15px;font-style: italic;">
				1. School of Electronic Information, Wuhan University, Wuhan 430079, China <br>
				2. Computer Vision Lab of ETH Zurich, Switzerland <br>
				3. Huawei Noah's Ark Lab, Shenzhen 518000, China <br>
				4. School of Computer Science, Wuhan University, Wuhan 430079, China <br>
				<!-- 3. Key Laboratory of Space Utilization, Technology and Engineering Center for Space Utilization, Chinese Academy of Sciences, Beijing 100094, China <br> -->
				<!-- 4. School of Electronic Information, Wuhan University, Wuhan 430072, China <br> -->
				<!-- 5. Faculty of Geo-Information Science and Earth Observation, University of Twente, Hengelosestraat 99, Enschede, Netherlands <br> -->
				<!-- 6. German Aerospace Center (DLR) and also Technical University of Munich, Germany -->
			</p>

		</div>

		<!-- <br><hr> -->

		<div class="row">
			<div class="span6 offset2">
				<ul class="nav nav-tabs">
					<br />
				</ul>
			</div>
		</div>


		<br>
		<div class="row">
			<div class="span12">
				<img src="files/motivations_part1.png" width="70%" class="img-responsive center-block" />
			</div>
		</div>

		<br>
		<br>

		<div class="row"> 
			<div class="span12">
				<h2 style="text-align:left; margin-bottom:-10px; margin-top:20px; ">
					Introduction 
				</h2>
				
				<!-- <div class="row">
					<div class="span6 offset2">
						<ul class="nav nav-tabs">
							<br />
						</ul>
					</div>
				</div> -->
				<br>
				<p style="text-align:justify; font-size: 17px">
					Even though the collaboration between traditional and neuromorphic event cameras brings prosperity to frame-event based vision applications, the performance is still confined by the resolution
					gap crossing two modalities in both spatial and temporal domains. This paper is devoted to bridging the gap by increasing the temporal resolution for images, i.e., motion deblurring, and the spatial
					resolution for events, i.e., event super-resolving, respectively. To this end, we introduce CrossZoom, a novel unified neural Network (CZ-Net) to jointly recover sharp latent sequences
					within the exposure period of a blurry input and the corresponding High-Resolution (HR) events. Specifically, we present multi-scale blur-event fusion architectures that leverage the scale-variant
					properties and effectively fuse cross-modality information to achieve cross-enhancement. Attention-based adaptive enhancement and cross-interaction prediction modules are devised to alleviate the
					distortions inherent in Low-Resolution (LR) events and enhance the final results through the prior blur-event complementary information. Furthermore, we propose a new dataset containing HR sharp
					image sequences and the corresponding real LR event streams to facilitate future research. Extensive qualitative and quantitative experiments on synthetic and real-world datasets demonstrate the
					effectiveness and robustness of the proposed method.
				</p>
				<img src="files/frameworks_v5.png" width="90%" class="img-responsive center-block" />
			</div>
		</div>

		<br>
		<br>
		<br>
		<div class="row">
			<div class="span12">
				<h2 id="scbm" style="text-align:left; margin-bottom:-10px; margin-top:20px;">
					CRDR Dataset
				</h2>
				<!-- <div class="row"> -->
					<!-- <div class="span6 offset2">
						<ul class="nav nav-tabs">
							<br />
						</ul>
					</div> -->
				<!-- </div> -->
				<br>
				<p style="text-align:justify; font-size: 17px">
					To our knowledge, no publicly released dataset is available yet for unified Event-based motion Deblurring and Super-Resolving (uEDSR) 
					tasks with Low-Resolution (LR) events. It motivates us to build a new dataset for Cross-Resolution Deblurring and Resolving (CRDR) containing paired HR sharp-blurry
					images and the corresponding HR-LR event streams. To simultaneously collect images and events, we build a hybrid camera system 
					composed of an LR DAVIS346 event camera of resolution 346x260 and an HR FLIR BlackFly U332S4 RGB camera of resolution 2048x1536 
					working at a frame rate of 118 FPS. A beam splitter connects two cameras to achieve minimum spatial parallax between RGB frames 
					and events. Since two cameras provide vision perceptions of different modalities and spatial resolutions, calibrations in both 
					spatial and temporal domains are essential to ensure alignments between collected RGB frames and events.
				</p>
				<img src="files/crdr_dataset_png.png" width="95%" class="img-responsive center-block" />
			</div>
		</div>

		<br>
		<br>
		<br>
		
		<div class="row">
			<div class="span12">
				<h2 id="scbm" style="text-align:left; margin-bottom:-10px; margin-top:20px;margin-down:-10px;">
					More Qualitative Results for MD and ESR
				</h2>
				<!-- <div class="row">
					<div class="span6 offset2">
						<ul class="nav nav-tabs">
							<br />
						</ul>
					</div>
				</div> -->
			</div>
		</div>
		
		<br>

		<section class="hero  is-small">
			<div class="container"> 
			<div id="results-carousel1" class="carousel results-carousel" style="width: 100%;">
				

				<div class="item item-toby">
				
					<video poster="" id="Stone" autoplay controls muted loop playsinline>
						<source src="./files/video/video_show_all_GOPRO1.mp4"
								type="video/mp4" style="clear:both;display:block;margin:auto">
					</video>
				</div>
						
				<div class="item item-toby">
				
					<video poster="" id="Stone" autoplay controls muted loop playsinline>
						<source src="./files/video/video_show_all_GOPRO2.mp4"
								type="video/mp4" style="clear:both;display:block;margin:auto">
					</video>
				</div>

				<div class="item item-toby">
				
					<video poster="" id="Stone" autoplay controls muted loop playsinline>
						<source src="./files/video/video_show_all_CRDD1.mp4"
								type="video/mp4" style="clear:both;display:block;margin:auto">
					</video>
				</div>

				<div class="item item-toby">
				
					<video poster="" id="Stone" autoplay controls muted loop playsinline>
						<source src="./files/video/video_show_all_CRDD2.mp4"
								type="video/mp4" style="clear:both;display:block;margin:auto">
					</video>
				</div>

			</div>
			</div>
		  </section>

		  <br>


		<br>
		<div class="row">
			<div class="span12">

		<div class="section bibtex">

			<h3 style="text-align:left; margin-bottom:10px; margin-top:20px; font-weight: bold">
				Citation
			</h3>
				<pre>
@misc{Zhang2023CrossZoom,
title={CrossZoom: Simultaneously Motion Deblurring and Event Super-Resolving},
author={Zhang, Chi and Zhang, Xiang and Lin, Mingyuan and Li, Cheng and He, Chu and Yang, Wen and Xia, Gui-Song and Yu, Lei},
year={2023},
journal={arXiv},
primaryClass={cs.CV}
}
</pre>
	                <br>
				</div>
				<br />
				<br />
				<br />
			</div>
		</div>

</body>
<script defer src="./static/js/fontawesome.all.min.js"></script>
<script src="./static/js/bulma-carousel.min.js"></script>
<script src="./static/js/bulma-slider.min.js"></script>
<script src="./static/js/index.js"></script>
<script src="./static/js/video_comparison.js"></script>
</html>
